# OVSeg

## 1.引言

解决方案：

​	1）

挖掘现有的图像字幕数据集>>提取字幕中的名词/根据预训练的分割模型生成类无关的mask区域建议>>预训练的CLIP>>最匹配的mask分配给每个提取的名词

​	2）

对mask图进行tokenizing: 用可学习的提示标记替换“zero-token”

微调: 仅训练提示，冻结CLIP

- 我们的分析表明，预训练的 CLIP 在掩模提案上表现不佳，使其成为两阶段方法的性能瓶颈。
- 我们从字幕中收集不同的掩模类别对，以适应掩模图像的 CLIP 并保留其泛化能力。
- 我们提出专门针对蒙版图像自适应的蒙版提示调整。该方法不会改变CLIP的权重，从而实现多任务权重共享。
- 我们首次证明开放词汇通才模型可以在 2017 年与有监督的专家模型的性能相媲美，而无需对数据集进行特定的调整

## 2.相关工作

LSeg（像素级）：将将像素嵌入与相应语义类的文本嵌入对齐，该语义类由 CLIP 的文本编码器生成。

OpenSeg （分割级）：提出通过区域词基础将分割级视觉特征与文本嵌入对齐。

## 3.方法

#### 3.1开放词汇语义分割的两阶段模型

MaskFormer（❌逐像素分割）：预测一组 N 个mask提案和相应的类预测

修改maskformer：对每个mask>>生成C维提议嵌入>>

- 如何训练通用且与类别无关的模型**（SAM）**来生成掩模建议是一个重要的主题，但超出了本文的范围

一个并行分支：

每个mask（前景1，背景0）>>包含所有前景的紧密边界框>>裁剪图像，遮盖背景>>调整大小，适应CLIP的分辨率>>融合两个类别预测

---

#### 3.2从字幕中收集不同的掩码类别对

自标记策略（Open-vocabulary image segmentation arXiv :2112.12143, 2021）（Regionclip）>>用于提取mask-类别对

maskformer>>提取mask建议

语言解析器（Natural language processing with Python: analyzing text with the natural language toolkit）>>句子里的名词>>

潜在的类>>CLIP分类

#### 3.3掩码提示调整

蒙版图像和自然图像之间最显着的区别是蒙版图像中的背景像素设置为零，导致许多"空白区域">>不包含有用的信息，而且还会给模型带来域分布偏移、性能下降>>**视觉提示调整**（Visual prompt tuning. arXiv preprint arXiv:2203.12119, 2022）

边界像素通常存在于部分遮蔽的patch中，对区域分类至关重要